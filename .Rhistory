scaled_training_set_2 = scale(training_set_2)
View(scaled_training_set_2)
matrice_cor = cor(scaled_training_set_2)
matrice_cor
View(matrice_cor)
library(caTools)
library(rpart)
library(rpart.plot)
tree = rpart(y~. = training_set_2)
tree = rpart(y~. , data = training_set_2)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
tree = rpart(y~. , data = training_set_2)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted) {
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test_set)
rmse(tree_pred, test_set$y)
Model_lda <- glm(y ~., family = binomial, data= training_set_2)
prediction = predict(model_lda, training_set, type="response")
model_lda <- glm(y ~., family = binomial, data= training_set_2)
prediction = predict(model_lda, training_set, type="response")
model_lda <- glm(y ~., family = binomial, data= training_set_2)
prediction = predict(model_lda, test_set, type="response")
model_qda <- qda(y~., data = training_set_2)
pred.qda <- predict(model_qda, newdata = test_set, type="response")
model_qda <- qda(y~., data = training_set_2)
library(ROCR)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(randomForest)
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
data["y"][data["y"] == "yes"] <- "1"
data["y"][data["y"] == "no"] <- "0"
data["y"]=transform(data["y"], y = as.numeric(y))
sapply(data["y"], mode)
data2 =data[,c("age","duration","campaign","pdays","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y")]
matrice_cor = cor(data2)
matrice_cor
model_lda <- glm(y ~., family = binomial, data= training_set_2)
prediction = predict(model_lda, test_set, type="response")
matrice_cor = cor(data2)
matrice_cor
model_lda <- glm(y ~., family = binomial, data= data2)
prediction = predict(model_lda, test, type="response")
model_qda <- qda(y~., data = data2)
pred.qda <- predict(model_qda, newdata = test, type="response")
View(test)
y = []
dim(test)
dim(test)
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
data <- na.omit(data)
matrice_cor = cor(data2)
matrice_cor
model_lda <- glm(y ~., family = binomial, data= data2)
prediction = predict(model_lda, test, type="response")
model_qda <- qda(y~., data = data2)
pred.qda <- predict(model_qda, newdata = test, type="response")
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(randomForest)
library(MASS)
model_qda <- qda(y~., data = data2)
pred.qda <- predict(model_qda, newdata = test, type="response")
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test_set)
rmse(tree_pred, test_set$y)
tree_pred = predict(tree, newdata = test)
rmse(tree_pred, test_set$y)
rf <- randomForest(factor(y) ~., data= data, proximity=TRUE)
print(rf)
y <- c()
test[,"y"] <- a.vector
View(test)
View(test)
View(test)
View(test)
View(test)
View(test)
View(data)
View(test)
test2 = test[,"y"]
test2 = cbind(test,y)
y <- c(0*1000)
y
y <- c(1:1000)
y
test2 = cbind(test,y)
View(test2)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test2)
rmse(tree_pred, test_set$y)
tree_pred = predict(tree, newdata = test2)
rmse(tree_pred, test2$y)
rf <- randomForest(factor(y) ~., data= data, proximity=TRUE)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(randomForest)
library(MASS)
data <- read.csv("data.csv",sep=",")
library(ggplot2)
library(randomForest)
library(MASS)
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
data <- na.omit(data)
data["y"][data["y"] == "yes"] <- "1"
data["y"][data["y"] == "no"] <- "0"
dim(test)
data["y"]=transform(data["y"], y = as.numeric(y))
sapply(data["y"], mode)
data2 =data[,c("age","duration","campaign","pdays","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y")]
y <- c(1:1000)
y
my.dataframe[ , "new.col"] <- a.vector
test2 = cbind(test,y)
matrice_cor = cor(data2)
matrice_cor
model_lda <- glm(y ~., family = binomial, data= data2)
prediction = predict(model_lda, test, type="response")
model_qda <- qda(y~., data = data2)
pred.qda <- predict(model_qda, newdata = test, type="response")
score_lda <- prediction(model_lda,test[,])
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test2)
rmse(tree_pred, test2$y)
rf <- randomForest(factor(y) ~., data= data, proximity=TRUE)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(ggplot2)
library(randomForest)
library(MASS)
data <- read.csv("data.csv",sep=",")
library(ggplot2)
library(randomForest)
View(data)
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(ggplot2)
library(randomForest)
library(MASS)
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
data <- na.omit(data)
data["y"][data["y"] == "yes"] <- "1"
data["y"][data["y"] == "no"] <- "0"
dim(test)
data["y"]=transform(data["y"], y = as.numeric(y))
sapply(data["y"], mode)
data2 =data[,c("age","duration","campaign","pdays","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y")]
y <- c(1:1000)
my.dataframe[ , "new.col"] <- a.vector
test2 = cbind(test,y)
matrice_cor = cor(data2)
model_lda <- glm(y ~., family = binomial, data= data2)
prediction = predict(model_lda, test, type="response")
model_qda <- qda(y~., data = data2)
pred.qda <- predict(model_qda, newdata = test, type="response")
score_lda <- prediction(model_lda,test[,])
performance(score,"auc")
score_lda <- prediction(model_lda,test[,20])
performance(score,"auc")
plot(performance(score,"tpr","fpr"),col="green")
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test2)
rmse(tree_pred, test2$y)
rf <- randomForest(factor(y) ~., data= data, proximity=TRUE)
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
library(randomForest)
library(ggplot2)
library(randomForest)
data <- read.csv('data.csv')
head(data)
# Check total number of rows and columns in the dataset.
nrow(data)
ncol(data)
# Drop all null values from the dataset.
data <- na.omit(data)
sum(is.na(data))
head(data)
# Divide the data into training and testing set. A split of 70% and 30% is used for training and testing respectively.
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data
data <- read.csv('data.csv')
test <- read.csv('test.csv')
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
library(ggplot2)
library(randomForest)
data <- read.csv('data.csv')
test <- read.csv('test.csv')
# Check total number of rows and columns in the dataset.
nrow(data)
ncol(data)
# Drop all null values from the dataset.
data <- na.omit(data)
sum(is.na(data))
head(data)
# Divide the data into training and testing set. A split of 70% and 30% is used for training and testing respectively.
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data
test <- test
# Initiate a random forest classifier. As the target variable is a binary variable hence we will factor it. All other features will be used to train the model.
rf <- randomForest(factor(y) ~., data=train, proximity=TRUE)
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
data <- na.omit(data)
print(rf)
data <- read.csv('data.csv')
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
library(ggplot2)
library(randomForest)
data <- read.csv('data.csv')
head(data)
# Check total number of rows and columns in the dataset.
nrow(data)
ncol(data)
# Drop all null values from the dataset.
data <- na.omit(data)
sum(is.na(data))
head(data)
# Divide the data into training and testing set. A split of 70% and 30% is used for training and testing respectively.
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.95, 0.05))
train <- data
data <- read.csv('data.csv')
head(data)
# Check total number of rows and columns in the dataset.
nrow(data)
ncol(data)
# Drop all null values from the dataset.
data <- na.omit(data)
sum(is.na(data))
head(data)
# Divide the data into training and testing set. A split of 70% and 30% is used for training and testing respectively.
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.95, 0.05))
train <- data[ind==1,]
test <- data[ind==2,]
# Initiate a random forest classifier. As the target variable is a binary variable hence we will factor it. All other features will be used to train the model.
rf <- randomForest(factor(y) ~., data=train, proximity=TRUE)
# Printing a summary of the trained model
print(rf)
# Perform predictions on the training set and print confusion matrix to see True positives, flase positives, true negatives, false negatives. We can see that accuracy score is 99.79% on training set.
p1 <- predict(rf, train)
confusionMatrix(p1, factor(train$y))
library(caTools)
library(rpart)
library(rpart.plot)
library(ROCR)
library(caret)
library(caret)
library(ROCR)
library(ggplot2)
library(randomForest)
library(randomForest)
library(MASS)
library(caret)
library(ggplot2)
library(randomForest)
library(MASS)
data <- read.csv("data.csv",sep=",")
test <- read.csv("test.csv",sep=",")
y <- c(nrows=1000)
y
test2 = cbind(test,y)
View(test2)
y <- vector(length = 1000)
y
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
tree_pred = predict(tree, newdata = test2)
rmse(tree_pred, data$y)
tree_pred = predict(tree, newdata = data)
rmse(tree_pred, data$y)
tree_pred = predict(tree, newdata = data)
rmse(tree_pred, data$y)
data["y"][data["y"] == "yes"] <- "1"
data["y"][data["y"] == "no"] <- "0"
data["y"]=transform(data["y"], y = as.numeric(y))
sapply(data["y"], mode)
tree = rpart(y~. , data = data)
plot(tree)
text(tree, pretty = 0)
title(main = "Regression Tree")
rpart.plot(tree)
prp(tree)
summary(tree)
printcp(tree)
plotcp(tree)
rmse = function(actual, predicted)
{
sqrt(mean((actual - predicted) ^ 2))
}
pred <- vector(length = 1000)
pred
tree_pred = predict(tree, newdata = data)
pred = tree_pred
rmse(pred, data$y)
library(randomForest)
tree_bagging = randomForest(y ~ ., data = data, mtry = 13,
importance = TRUE, ntrees = 2)
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
library(ggplot2)
library(randomForest)
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
# We have applied random forest classifier on the provided dataset. I will explain each and every step one by one.
# Importing required libraries.
library(caret)
library(randomForest)
library(ggplot2)
library(randomForest)
data <- read.csv('data.csv')
dim(data)
str(data)
summary(data)
# Drop all null values from the dataset.
data <- na.omit(data)
sum(is.na(data))
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
# Initiate a random forest classifier. As the target variable is a binary variable hence we will factor it. All other features will be used to train the model.
rf <- randomForest(factor(y) ~., data=train, proximity=TRUE)
# Printing a summary of the trained model
print(rf)
# Perform predictions on the training set and print confusion matrix to see True positives, flase positives, true negatives, false negatives. We can see that accuracy score is 99.79% on training set.
p1 <- predict(rf, train)
confusionMatrix(p1, factor(train$y))
# Use 30% test data for predictions.
p2 <- predict(rf, test)
# Calcualte a confusion matrix and we can see that the accuracy on the test set is 91.38% which is good.
confusionMatrix(p2, factor(test$y))
# Plot the most important features of the dataset.
varImpPlot(rf)
test_data <- read.csv('test.csv')
# Perform prediction on given test sets.
p2 <- predict(rf, test_data)
# Display classification results.
table(p2)
predictions = data.frame(id=rownames(test), y=p2)
write.csv(p2 , file = "BENYEMNA Hamza & BITAR Aref & AZZOUZI Hajar - DIA2 groupe13 - predictions .csv", row.names = F)
to_be_submitted = data.frame(id=rownames(test), p2)
View(test)
to_be_submitted = data.frame(id=rownames(test_data), p2)
write.csv(to_be_submitted , file = "BENYEMNA Hamza & BITAR Aref & AZZOUZI Hajar - DIA2 groupe13 - predictions .csv", row.names = F)
View(to_be_submitted)
to_be_submitted = data.frame(id=rownames(test_data), p2)
write.csv(to_be_submitted , file = "BENYEMNA Hamza & BITAR Aref & AZZOUZI Hajar - DIA2 groupe13 - predictions .csv", row.names = F)
# Use 30% test data for predictions.
y <- predict(rf, test)
# Calcualte a confusion matrix and we can see that the accuracy on the test set is 91.38% which is good.
confusionMatrix(y, factor(test$y))
# Plot the most important features of the dataset.
varImpPlot(rf)
test_data <- read.csv('test.csv')
head(test_data)
# Perform prediction on given test sets.
y <- predict(rf, test_data)
# Display classification results.
table(p2)
to_be_submitted = data.frame(id=rownames(test_data), y)
write.csv(to_be_submitted , file = "BENYEMNA Hamza & BITAR Aref & AZZOUZI Hajar - DIA2 groupe13 - predictions .csv", row.names = F)
str(data)
str(data)
summary(data)
# Calculate a confusion matrix and we can see that the accuracy on the test set is 91.38% which is good.
confusionMatrix(y, factor(test$y))
# Calculate a confusion matrix and we can see that the accuracy on the test set is 91.38% which is good.
confusionMatrix(y, factor(test$y))
# Importing required libraries.
library(caret)
library(ggplot2)
library(randomForest)
data <- read.csv('data.csv')
dim(data)
str(data)
summary(data)
# Drop all null values from the dataset.
data <- na.omit(data)
# Check if null values are removed. If the sum is 0 then that means no null values are present.
sum(is.na(data))
ind <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]
train <- data[ind==1,]
test <- data[ind==2,]
# Initiate a random forest classifier. As the target variable is a binary variable hence we will factor it. All other features will be used to train the model.
rf <- randomForest(factor(y) ~., data=train, proximity=TRUE)
# Printing a summary of the trained model
print(rf)
# Perform predictions on the training set and print confusion matrix to see True positives, flase positives, true negatives, false negatives. We can see that accuracy score is 99.79% on training set.
p1 <- predict(rf, train)
confusionMatrix(p1, factor(train$y))
# Use 30% test data for predictions.
y <- predict(rf, test)
# Calculate a confusion matrix and we can see that the accuracy on the test set is 91.38% which is good.
confusionMatrix(y, factor(test$y))
# Plot the most important features of the data set.
varImpPlot(rf)
test_data <- read.csv('test.csv')
head(test_data)
# Perform prediction on given test sets.
y <- predict(rf, test_data)
# Display classification results.
table(p2)
to_be_submitted = data.frame(id=rownames(test_data), y)
write.csv(to_be_submitted , file = "BENYEMNA Hamza & BITAR Aref & AZZOUZI Hajar - DIA2 groupe13 - predictions .csv", row.names = F)
